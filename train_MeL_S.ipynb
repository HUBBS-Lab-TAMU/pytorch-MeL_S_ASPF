{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We tried to set the random seeds but we found that the results may still vary. As a result, we run every experiment 10 times and get the average result.\n",
    "# This is the sample running results of method MeL-S as mentioned in the paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# import pickle\n",
    "from dataset_classification import Train, Test\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "from dataset_SNN import Train_Siamese\n",
    "from model_Siamese import Siamese\n",
    "from model_classification import FNN\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import sys\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "\n",
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "def max_key(score):  \n",
    "     v=list(score.values())\n",
    "     k=list(score.keys())\n",
    "     return k[v.index(max(v))]\n",
    "    \n",
    "def run(model_name):\n",
    "    trainID = ['Ses01F', 'Ses01M', 'Ses02F', 'Ses02M', 'Ses03F', 'Ses03M', 'Ses04F', 'Ses04M', 'Ses05F', 'Ses05M']\n",
    "    print (model_name)\n",
    "    \n",
    "    # g is the number of samples per speaker per emotion\n",
    "    for g in range(1, 11):\n",
    "        score_all = []\n",
    "        \n",
    "        # because we did not specify the random seeds, we run 10 times and calculate the average\n",
    "        for l in range(0, 10):\n",
    "            num_train = g\n",
    "            num_speaker = 5\n",
    "            gender = ['F', 'M']\n",
    "\n",
    "            lr = 0.0005\n",
    "            max_iter = 250\n",
    "            model_path = 'SNN_source_models/'+model_name\n",
    "\n",
    "            # load the data for the Siamese nn\n",
    "            trainSet = Train_Siamese('feature/iemocap_feature_processed.csv', trainID = trainID, num_sample = g, epoch = max_iter*128)\n",
    "            trainLoader = DataLoader(trainSet, batch_size=128, shuffle=False)\n",
    "\n",
    "            loss_fn = torch.nn.BCELoss(size_average=True)\n",
    "\n",
    "            net = Siamese()\n",
    "            net.load_state_dict(torch.load(model_path))\n",
    "\n",
    "            net.train()\n",
    "\n",
    "            optimizer = torch.optim.Adam(net.parameters(),lr = lr )\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            train_loss = []\n",
    "            loss_val = 0\n",
    "\n",
    "            for batch_id, (data1, data2, label, prob_return) in enumerate(trainLoader, 1):\n",
    "                if batch_id > max_iter:\n",
    "                    break\n",
    "                data1, data2, label = Variable(data1), Variable(data2), Variable(label)\n",
    "                optimizer.zero_grad()\n",
    "                output = net.forward(data1, data2)\n",
    "                loss = loss_fn(output, label)\n",
    "                loss_val += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "#                     # update the prob (likelihood in the paper) every 10 iterations\n",
    "#                     if batch_id % 10 == 0:\n",
    "#                         prob_tmp = []\n",
    "#                         increase_tmp = []\n",
    "#                         for k in range(0, len(output)):\n",
    "#                             # if ((output[k][0] < 0.5 and label[k][0] == 1) or (output[k][0] > 0.5 and label[k][0] == 0)):\n",
    "#                             prob_tmp.append(prob_return[k].tolist())\n",
    "#                             increase_tmp.append( (abs(output[k][0] - label[k][0])*1).item() )              \n",
    "#                         trainSet.update_prob(prob_list = prob_tmp, prob_increase = increase_tmp)\n",
    "\n",
    "\n",
    "\n",
    "            # initilize a new model and copy the weight of trained SNN to the new model\n",
    "            model = FNN()\n",
    "            pretrained_dict = net.state_dict()\n",
    "            model_dict = model.state_dict()\n",
    "            pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "            model_dict.update(pretrained_dict) \n",
    "            model.load_state_dict(model_dict)\n",
    "\n",
    "            # fine-tuning the model in the supervised way (same as MeL-S)\n",
    "            trainSet = Train('feature/iemocap_feature_processed.csv', trainID = trainID, num_sample = g, epoch = 200)\n",
    "            trainLoader = DataLoader(trainSet, batch_size=32, shuffle=False)\n",
    "            testSet = Test(datas_train = trainSet.datas_train, datas_test = trainSet.datas_test, num_classes = 3, trainID = trainID, num_sample = g)\n",
    "            testLoader = DataLoader(testSet, batch_size=1, shuffle=False)\n",
    "\n",
    "            loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "            optimizer = torch.optim.Adam(model.parameters(),lr = 0.001 )\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "\n",
    "            for batch_id, (data, label) in enumerate(trainLoader, 0):\n",
    "                if batch_id > max_iter:\n",
    "                    break\n",
    "                data, label = Variable(data), Variable(label)\n",
    "                optimizer.zero_grad()\n",
    "                output = model.forward(data)\n",
    "                loss = loss_fn(output, label.squeeze().type(torch.LongTensor))\n",
    "                loss_val += loss.item()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            correct_list = []\n",
    "            pred_list = []\n",
    "            for test_id, (test, label) in enumerate(testLoader, 0):\n",
    "                # print (test_id)\n",
    "                test = Variable(test)\n",
    "                output = model.forward(test).data.cpu().numpy()[0]\n",
    "                predict = output.argmax()\n",
    "                pred_list.append(predict)\n",
    "                correct_list.append(int(label[0]))\n",
    "\n",
    "\n",
    "            score_all.append(metrics.recall_score(correct_list, pred_list, average='macro'))\n",
    "\n",
    "        print ('num of speaker:', g, end = ' ')\n",
    "        print(sum(score_all)/len(score_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enterface_base.pth\n",
      "num of speaker: 1 0.6431494024490615\n",
      "num of speaker: 2 0.6660534293699456\n",
      "num of speaker: 3 0.6703474639945772\n",
      "num of speaker: 4 0.6776208090056939\n",
      "num of speaker: 5 0.6880217435448688\n",
      "num of speaker: 6 0.7058977521044496\n",
      "num of speaker: 7 0.6910009614359881\n",
      "num of speaker: 8 0.6911679160835706\n",
      "num of speaker: 9 0.7065210364978425\n",
      "num of speaker: 10 0.7140275325945178\n"
     ]
    }
   ],
   "source": [
    "run('enterface_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crema_d_base.pth\n",
      "num of speaker: 1 0.6527660419486876\n",
      "num of speaker: 2 0.6853930109838234\n",
      "num of speaker: 3 0.710318820266236\n",
      "num of speaker: 4 0.7167962872865705\n",
      "num of speaker: 5 0.7223353297497266\n",
      "num of speaker: 6 0.7263359796131653\n",
      "num of speaker: 7 0.7134556818541636\n",
      "num of speaker: 8 0.7343079303644416\n",
      "num of speaker: 9 0.7502104241468316\n",
      "num of speaker: 10 0.738815821347587\n"
     ]
    }
   ],
   "source": [
    "run('crema_d_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iemocap_base.pth\n",
      "num of speaker: 1 0.6801168003377236\n",
      "num of speaker: 2 0.6917276857797304\n",
      "num of speaker: 3 0.7162855333992315\n",
      "num of speaker: 4 0.7197790782719125\n",
      "num of speaker: 5 0.7136124562631778\n",
      "num of speaker: 6 0.7117803743408591\n",
      "num of speaker: 7 0.7117478962389086\n",
      "num of speaker: 8 0.718206101550203\n",
      "num of speaker: 9 0.7185453170095607\n",
      "num of speaker: 10 0.6998388715611427\n"
     ]
    }
   ],
   "source": [
    "run('iemocap_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ravdess_base.pth\n",
      "num of speaker: 1 0.6929291640668469\n",
      "num of speaker: 2 0.7011937139393326\n",
      "num of speaker: 3 0.694432334179494\n",
      "num of speaker: 4 0.719270577110825\n",
      "num of speaker: 5 0.7105926506095628\n",
      "num of speaker: 6 0.7008226584311161\n",
      "num of speaker: 7 0.7126698575204146\n",
      "num of speaker: 8 0.7265481969989148\n",
      "num of speaker: 9 0.7225179574085336\n",
      "num of speaker: 10 0.7252871323993715\n"
     ]
    }
   ],
   "source": [
    "run('ravdess_base.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
